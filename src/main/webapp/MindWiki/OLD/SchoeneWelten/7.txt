ein gedankliches Hilfsmittel, um das WoZu klarer fassen zu können:
* wähle abhängig von Deiner LebensErwartung ein exaktes Datum in der ZuKunft
* beschreibe eine exemplarische "ideale Situation" an diesem Datum,
* analysiere das kausale Netzwerk aus NextAction's und ProJect's, welches aus der GegenWart in SchoeneWelten führt
* DoIt

Beispiel 1: IceCream2019



Beispiel 2: SchoeneWelten Richtung InfoMorph:
RainerWasserfuhr wird [diesem Chart zufolge|http://www.flickr.com/photos/rainerw/2326936072/] das
Jahr 2029 zu ca. 88% erleben:
||UnTil||LebensErwartung
|2019|95%
|2029|88%
|2039|75%
|2049|55%
|2059|22%
|2069|2%


UnTil2029 wird es zu 80% eine mit MindEyes vergleichbare Technologie geben.
Und eine komplette SpiegelWelt.

Nicht ganz sicher ist, wie weit bis dahin das BrainComputerInterface fortgeschritten sein wird.
Ziemlich sicher und technologisch auch mit heutigen Mitteln fast schon realisierbar ist
die umfassende digitale PerCeive-Filterung (Augen, Ohren, Temperatur, Duft
und chemische Spurenelemente, Bewegungen, ganz zu schweigen
von neuen Sinneskanälen wie Infrarotkameras).
Alle diese Sinnesdaten können aufgezeichnet werden. Zu Kosten, die sich bis dahin
EarlyAdopter, aber vielleicht auch schon JederMann leisten können.
Doch dieser Datenstrom der SpiegelWelt wird nur eine äussere Hülle sein.
Der Körper und überhaupt der Aufenthaltsort eines derart omnipräsenten ConScious'ness
wird immer irrelevanter. BodyOne ist nur noch ein AvaTar, den ich neben vielen
anderen beobachten und seine Funktion sicherstellen kann.
Im Kern geht es um das semantische Modell, das aus den Sinnesdaten EmerGier't:
Personen, Orte, Ereignisse, Themen, Sprechakte, StateMent's.
Aus diesen werden Pattern gebildet. Das biologische Gehirn verlässt
sich immer mehr auf sein LifeWiki, dass ja jetzt auch bei jedem PerCeive
eingebunden ist. Die FaceBase erkennt automatisch Gesichter.
Mittels VoiceBase kann die Umgebung nach bekannten Stimmen durchgescreent werden.
